{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import MeanShift, DBSCAN, estimate_bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(join('..', 'data', 'data_preprocessed.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "# Remember which metric_features we decided to keep?\n",
    "metric_features = ['income',\n",
    " 'frq',\n",
    " 'rcn',\n",
    " 'clothes',\n",
    " 'kitchen',\n",
    " 'small_appliances',\n",
    " 'toys',\n",
    " 'house_keeping',\n",
    " 'per_net_purchase',\n",
    " 'spent_online']\n",
    "\n",
    "non_metric_features = df.columns[df.columns.str.startswith('oh_')].tolist() # CODE HERE\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')].tolist()  # CODE HERE\n",
    "\n",
    "unused_features = [i for i in df.columns if i not in (metric_features+non_metric_features+pc_features) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('metric_features:', metric_features)\n",
    "print('\\nnon_metric_features:', non_metric_features)\n",
    "print('\\nunused_features:', unused_features)\n",
    "print('\\npc_features:', pc_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Based Clustering\n",
    "## Mean Shift Clustering\n",
    "What is Mean-shift clustering? How does it work?\n",
    "\n",
    "Single seed             |  Multiple seeds\n",
    ":-------------------------:|:-------------------------:\n",
    "![](../figures/clustering/mean_shift_0.gif)  |  ![](../figures/clustering/mean_shift_tutorial.gif)\n",
    " \n",
    "### Characteristics:\n",
    "- No need to define number of clusters apriori\n",
    "- Can detect clusters of any shape\n",
    "- Robust to outliers\n",
    "- Depends on the bandwidth hyperparameter (but there's a way to estimate it)\n",
    "- **Main drawback**: Poor scalability (on both the algorithm and in estimating the bandwidth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following bandwidth can be automatically detected using \n",
    "# # (we need to set quantile though)\n",
    "# Based on distance to nearest neighbors for all observations\n",
    "\n",
    "bandwidth = estimate_bandwidth(df[metric_features], \n",
    "# TO-DO: manipulate the quantile value such that we obtain a small enough bandwidth\n",
    "                               quantile=0.9, # \n",
    "                               random_state=1, \n",
    "                               n_jobs=-1)\n",
    "bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does `estimate_bandwidth` work?\n",
    "\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.cluster.estimate_bandwidth.html#sklearn.cluster.estimate_bandwidth\n",
    "\n",
    "Code from https://github.com/scikit-learn/scikit-learn/blob/6e9039160/sklearn/cluster/_mean_shift.py#L43\n",
    "\n",
    "```python\n",
    "    \n",
    "    n_neighbors = int(X.shape[0] * quantile)\n",
    "    if n_neighbors < 1:  # cannot fit NearestNeighbors with n_neighbors = 0\n",
    "        n_neighbors = 1\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, n_jobs=n_jobs)\n",
    "    nbrs.fit(X)\n",
    "\n",
    "    bandwidth = 0.0\n",
    "    for batch in gen_batches(len(X), 500):\n",
    "        d, _ = nbrs.kneighbors(X[batch, :], return_distance=True)\n",
    "        bandwidth += np.max(d, axis=1).sum()\n",
    "\n",
    "    return bandwidth / X.shape[0]\n",
    "```\n",
    "\n",
    "\n",
    "1. Specify `quantile`: \n",
    "    - Determines `n_neighbors`, which is how many neighbors to include when computing distances\n",
    "2. Divide data into batches (for efficient calculation)\n",
    "3. For each point, calculate `n_neighbors` Nearest Neighbors.\n",
    "4. Get the largest distance to the `n_neighbors`\n",
    "5. Get the average of these largest distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Shift Algorithm, changing bandwidth values by modifying quantile values:\n",
    "\n",
    "![](../figures/clustering/ms_quantile_bandwidth.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Shift Algorithm, showing initial and final locations of seeds with varying bandwidth values.\n",
    "\n",
    "![](../figures/clustering/ms_q_bw_start_end.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mean-shift clustering with bandwidth set using estimate_bandwidth\n",
    "# TO-DO: explore the MeanShift class and obtain the cluster labels\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift\n",
    "\n",
    "ms = MeanShift()\n",
    "ms_labels = None\n",
    "\n",
    "ms_n_clusters = len(np.unique(ms_labels))\n",
    "print(\"Number of estimated clusters : %d\" % ms_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([df[metric_features], pd.Series(ms_labels, index=df.index, name=\"ms_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the R^2 of the cluster solution\n",
    "sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "ssw_labels = df_concat.groupby(by='ms_labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "r2 = ssb / sst\n",
    "print(\"Cluster solution with R^2 of %0.4f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density-based spatial clustering of applications with noise)\n",
    "What is DBSCAN clustering? How does it work?\n",
    "\n",
    "DBSCAN animation            |  Core, border and noise\n",
    ":-------------------------:|:-------------------------:\n",
    "![](../figures/clustering/dbscan.gif)  |  ![](../figures/clustering/dbscan.jpg)\n",
    "\n",
    "\n",
    "### Characteristics:\n",
    "- No need to define number of clusters apriori\n",
    "- Resistant to noise and outliers\n",
    "- Can identify outliers\n",
    "- Can handle clusters of different shapes and sizes\n",
    "- Depends highly on the epsilon hyperparameter and it can be hard to tune\n",
    "- Does not work well with clusters of varying densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform DBSCAN clustering\n",
    "# TO-DO: explore the DBSCAN class and obtain the cluster labels\n",
    "dbscan = DBSCAN()\n",
    "dbscan_labels = None \n",
    "\n",
    "dbscan_n_clusters = len(np.unique(dbscan_labels))\n",
    "print(\"Number of estimated clusters : %d\" % dbscan_n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN Algorithm, varying epsilon and minpts values:\n",
    "\n",
    "*Square markers are noise points* \n",
    "\n",
    "![](../figures/clustering/dbscan_eps_minpts.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining eps and min_samples:\n",
    "- **MinPts**: As a rule of thumb, **minPts = 2 x dim** can be used, but it may be necessary to choose larger values for very large data, for noisy data or for data that contains many duplicates.\n",
    "\n",
    "- **ε**: The value for ε can then be chosen by using a **k-distance graph**, plotting the distance to the kth (k = minPts - 1) nearest neighbor ordered from the largest to the smallest value. Good values of ε are where this plot shows an **\"elbow\"**: if ε is chosen much too small, a large part of the data will not be clustered; whereas for a too high value of ε, clusters will merge and the majority of objects will be in the same cluster. \n",
    "\n",
    "- The assumption is that for points in a cluster, **their k nearest neighbors are at roughly the same distance**. Noise points have their k-th nearest neighbors at farther distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-distance graph to find out the right eps value\n",
    "neigh = NearestNeighbors(n_neighbors=20)\n",
    "neigh.fit(df[metric_features])\n",
    "distances, _ = neigh.kneighbors(df[metric_features])\n",
    "distances = np.sort(distances[:, -1])\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([df[metric_features], pd.Series(dbscan_labels, index=df.index, name=\"dbscan_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting noise (potential outliers)\n",
    "# TO-DO: can we identify the noisy data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the R^2 of the cluster solution\n",
    "df_nonoise = df_concat.loc[df_concat['dbscan_labels'] != -1]\n",
    "sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "ssw_labels = df_nonoise.groupby(by='dbscan_labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "r2 = ssb / sst\n",
    "print(\"Cluster solution with R^2 of %0.4f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why did the DBSCAN gave us just one cluster?\n",
    "- What can we do with the noisy data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM (Gaussian Mixture Model )\n",
    "\n",
    "What is GMM? How does it work?\n",
    "\n",
    "![](../figures/clustering/gmm.gif)\n",
    "\n",
    "--- \n",
    "\n",
    "$$\\mathcal{p(\\vec{x})} \\ = \\ \\sum_{i=1}^K \\phi_i \\mathcal{N}(\\vec{x}|\\vec{\\mu_i}, \\Sigma_i) \\tag{eq1}$$\n",
    "$$\\mathcal{N}(\\vec{x}|\\vec{\\mu_i}, \\Sigma_i) \\ = \\ \\frac{1}{\\sqrt{{(2\\pi)}^{K}|\\Sigma_i|}}e^{-\\frac{1}{2} (\\vec{x} - \\vec{\\mu_i})^T \\Sigma_i^{-1} (\\vec{x} - \\vec{\\mu_i})} \\tag{eq2}$$\n",
    "$$\\sum_{i=1}^K \\phi_i \\ = \\ 1 \\tag{eq3}$$\n",
    "\n",
    ", where:\n",
    "- $\\phi_i$ is the component weight (scalar) for Component $i$ (probability of an observation being generated by Component $i$)\n",
    "- $\\vec{\\mu_i}$ is the mean vector for Component $i$,\n",
    "- $\\Sigma_i$ is the Covariance matrix for Component $i$\n",
    "\n",
    "---\n",
    "\n",
    "- **(eq1)** gives the probability of a point $x$ given the estimated Gaussian mixture\n",
    "- **(eq2)** is the probability density function of a multivariate Gaussian with mean $\\vec{\\mu_i}$ and covariance $\\Sigma_i$\n",
    "- **(eq3)** states that the sum of the component weights is 1, such that the total probability distribution normalizes to 1\n",
    "\n",
    "### Characteristics:\n",
    "- Assumes the data is generated from a mixture of finite number of Gaussian distributions with unknown parameters\n",
    "- Use the EM (Expectation Maximization algorithm) to estimate the parameters\n",
    "- Provides a probability of each observation belonging to each cluster\n",
    "- Advantages over K-Means:\n",
    "    - Can deal with spherical and elipsoid cluster shapes\n",
    "    - Number of components needs to be defined apriori\n",
    "    \n",
    "\n",
    "[Read More](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing GMM clustering\n",
    "# https://scikit-learn.org/1.5/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4, \n",
    "                      covariance_type='full', \n",
    "                      n_init=10, \n",
    "                      init_params='kmeans', \n",
    "                      random_state=1)\n",
    "gmm_labels = gmm.fit_predict(df[metric_features])\n",
    "labels_proba = gmm.predict_proba(df[metric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at the estimated parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated component weights\n",
    "gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated mean vectors of the Components\n",
    "print(gmm.means_.shape)\n",
    "gmm.means_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimated covariance matrices of the Components\n",
    "gmm.covariances_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining covariance_type:\n",
    "This hyperparameter controls the **degrees of freedom** in the shape of each cluster The more degrees of freedom we have the more complex shapes the model can fit and the more computationally expensive the model will be.\n",
    "\n",
    "![](../figures/clustering/gmm_covariance.png)\n",
    "\n",
    "- `covariance_type=\"tied\"` makes all components share the same general covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../figures/clustering/gmm_components_covars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining n_components:\n",
    "\n",
    "**AIC**: estimates the relative amount of information lost by a model used to represent the data-generation process. The smaller the better.\n",
    "\n",
    "**BIC**: similar to AIC but penalizes more complex models (i.e. favors simpler models). The smaller the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while to run\n",
    "# \n",
    "# Selecting number of components based on AIC and BIC\n",
    "n_components = np.arange(1, 16)\n",
    "models = [GaussianMixture(n, covariance_type='full', n_init=10, random_state=1).fit(df[metric_features])\n",
    "          for n in n_components]\n",
    "\n",
    "bic_values = [m.bic(df[metric_features]) for m in models]\n",
    "aic_values = [m.aic(df[metric_features]) for m in models]\n",
    "plt.plot(n_components, bic_values, label='BIC')\n",
    "plt.plot(n_components, aic_values, label='AIC')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')\n",
    "plt.xticks(n_components)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the AIC and BIC measures can also be used to select diferent hyperparameters such as the covariance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing GMM clustering\n",
    "gmm = GaussianMixture(n_components=3, covariance_type='full', n_init=10, init_params='kmeans', random_state=1)\n",
    "gmm_labels = gmm.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the labels to df\n",
    "df_concat = pd.concat([df[metric_features], pd.Series(gmm_labels, index=df.index, name=\"gmm_labels\")], axis=1)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the R^2 of the cluster solution\n",
    "sst = get_ss(df[metric_features])  # get total sum of squares\n",
    "ssw_labels = df_concat.groupby(by='gmm_labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "r2 = ssb / sst\n",
    "print(\"Cluster solution with R^2 of %0.4f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by Perspectives\n",
    "- Demographic Perspective:\n",
    "- Value Perspective:\n",
    "- Product Perspective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the Perspectives\n",
    "- How can we merge different cluster solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
